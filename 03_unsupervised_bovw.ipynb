{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bag of Visual Words (Features)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import copyreg\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "with open(\"data/SCUT-FBP5500_v2/train_test_files/split_of_60%training and 40%testing/train.txt\",\"r\") as train:\n",
    "    train_info = [l.split() for l in train.readlines()]\n",
    "    train_info = {k:v for k,v in train_info}\n",
    "    #image_list = [c[0] for c in train_info]\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "with open(\"data/SCUT-FBP5500_v2/train_test_files/split_of_60%training and 40%testing/test.txt\",\"r\") as test:\n",
    "    test_info = [l.split() for l in test.readlines()]\n",
    "    test_info = {k:v for k,v in test_info}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# let's load in all our dataframes ( 10gb)\n",
    "path = 'data/SCUT-FBP5500_v2/reduced/'\n",
    "fl = sorted(os.listdir(path))\n",
    "fl.remove('.DS_Store') \n",
    "\n",
    "comp_df = pd.DataFrame()\n",
    "for _, p in enumerate(fl):\n",
    "    comp_df = pd.concat([comp_df,pd.read_pickle(path + p)])\n",
    "print(f'done')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# this cell is going to go through each row and check if the filename is in the train list\n",
    "train_df_list = []\n",
    "test_df_list = []\n",
    "for ix, row in comp_df.iterrows():\n",
    "    if row[0] in train_info.keys():\n",
    "        train_df_list.append(row)\n",
    "    else:\n",
    "        test_df_list.append(row)\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# okay let's check the length to make sure we grabbed everything\n",
    "len(train_df_list) == len(train_info)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "train_df = pd.DataFrame(train_df_list, columns=comp_df.columns)\n",
    "test_df = pd.DataFrame(test_df_list, columns=comp_df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# now to delete the comp_df\n",
    "del comp_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "train_df.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Filename', 'orb_kp', 'orb_dec', 'male', 'asian', 'PCA_1', 'PCA_2'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "train_df['orb_dec'].sample()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    [[218, 194, 44, 191, 13, 213, 156, 173, 140, 2...\n",
       "Name: orb_dec, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# extract the keypoint_desc vectors and append them to a list\n",
    "def orb_extractor(df):\n",
    "    image_vectors = {}\n",
    "    descriptor_list =[]\n",
    "    for _, row in df.iterrows():\n",
    "        descriptor_list.extend(row['orb_dec'])\n",
    "        image_vectors[row['Filename']] = row['orb_dec']\n",
    "    return (image_vectors, descriptor_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# okay now we are going to extract what we need for feature engineering\n",
    "train_orb_dict, train_orbs = orb_extractor(train_df)\n",
    "test_orb_dict, test_orbs = orb_extractor(test_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# before we begin let's save some files\n",
    "\n",
    "with open('data/SCUT-FBP5500_v2/train_bovw_list', 'wb') as d:\n",
    "    pickle.dump(train_orbs,d)\n",
    "    \n",
    "with open('data/SCUT-FBP5500_v2/train_orb_dict', 'wb') as d:\n",
    "    pickle.dump(train_orb_dict,d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# We have 631,264 feature vectors (words) for our images\n",
    "## time to cluster them with Kmeans; unsupervised classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_orbs = pickle.load(open('data/SCUT-FBP5500_v2/train_bovw_list', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_orb_dict = pickle.load(open('data/SCUT-FBP5500_v2/train_orb_dict', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# A k-means clustering algorithm who takes 2 parameter which is number \n",
    "# of cluster(k) and the other is descriptors list(unordered 1d array)\n",
    "# Returns an array that holds central points.\n",
    "def kmeans(k, descriptor_list):\n",
    "    kmeans = KMeans(n_clusters = k, n_init=10)\n",
    "    kmeans.fit(descriptor_list)\n",
    "    visual_words = kmeans.cluster_centers_ \n",
    "    return visual_words\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Takes the central points which is visual words \n",
    "# Since this is one subject we can greatly reduce this space\n",
    "# let's start with the # of original pts ie 86 \n",
    "visual_words = kmeans(86, train_orbs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# now let's assume everyone a single identifying property\n",
    "# this K is too high\n",
    "#visual_words_2200 = kmeans(2200, train_orbs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "visual_words_500 = kmeans(500, train_orbs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#with open('data/SCUT-FBP5500_v2/visual_words_500_centers', 'wb') as d:\n",
    "#    pickle.dump(visual_words_500,d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "type(visual_words_500)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Now time for the histogram plotting to analyze BOVW\n",
    "\n",
    "# Takes 2 parameters. The first one is a dictionary that holds the descriptors that are separated by image \n",
    "# And the second parameter is an array that holds the central points (visual words) of the k means clustering\n",
    "# Returns a dictionary that holds the histograms for each images. \n",
    "def image_class_normed(all_bovw, centers):\n",
    "    dict_keys = []\n",
    "    feats = []\n",
    "    for key,value in all_bovw.items():\n",
    "        dict_keys.append(key)\n",
    "        # obtains distance/closeness to centers for keypoints\n",
    "        dist = distance.cdist(value, centers, metric='euclidean')\n",
    "        \n",
    "        # argmin for each of key points, get the closest feature vocab (center)\n",
    "        bin_assignment = np.argmin(dist, axis=1)\n",
    "        \n",
    "        # classify each kp into symbols\n",
    "        # create histogram with size N describing number of symbols\n",
    "        histogram = np.zeros(len(centers))\n",
    "        for id_assign in bin_assignment:\n",
    "            histogram[id_assign] += 1\n",
    "        \n",
    "        # assign the histogram to global features    \n",
    "        feats.append(histogram)\n",
    "    \n",
    "    # normalize \n",
    "    feats = np.asarray(feats)\n",
    "    feats_norm = np.linalg.norm(feats,axis=1)\n",
    "    for i in range(0, feats.shape[0]):\n",
    "        feats[i] = feats[i] /feats_norm[i]\n",
    "        \n",
    "    # feats now holds all the image features\n",
    "    feat_dict = {k:v for k,v in zip(dict_keys, feats)}\n",
    "\n",
    "    return feat_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Creates histograms for train data  \n",
    "# returns dict with image as key then a matrix of visual features  \n",
    "bovw_train = image_class_normed(train_orb_dict, visual_words_500) \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "# Creates histograms for test data\n",
    "bovw_test = image_class_normed(test_orb_dict, visual_words_500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "train_df['rating'] = train_df['Filename'].apply(lambda x: train_info[x])\n",
    "train_df['bovw'] = train_df['Filename'].apply(lambda x: bovw_train[x])\n",
    "\n",
    "# assign bovw feature to test df\n",
    "test_df['bovw'] = test_df['Filename'].apply(lambda x: bovw_test[x])\n",
    "test_df['rating'] = test_df['Filename'].apply(lambda x: test_info[x])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "train_df_flt = train_df[['Filename', 'male', 'asian','bovw','PCA_1', 'PCA_2','rating']]\n",
    "test_df_flt = test_df[['Filename', 'male', 'asian','bovw','PCA_1', 'PCA_2','rating']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# save our files\n",
    "with open('data/SCUT-FBP5500_v2/train_df', 'wb') as d:\n",
    "    pickle.dump(train_df_flt,d)\n",
    "    \n",
    "with open('data/SCUT-FBP5500_v2/test_df', 'wb') as d:\n",
    "    pickle.dump(test_df_flt,d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "train_df_flt.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2200, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "test_df_flt.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3301, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}