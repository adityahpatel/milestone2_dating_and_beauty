{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ML Notebook 3: Unsupervised Learning, Bag of Visual Features (\"Words\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import copyreg\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tqdm import tqdm "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# First thing that needs to be done is to split the data into our sets\n",
    "\n",
    "# Training\n",
    "with open(\"data/SCUT-FBP5500_v2/train_test_files/split_of_60%training and 40%testing/train.txt\",\"r\") as train:\n",
    "    train_info = [l.split() for l in train.readlines()]\n",
    "    train_info = {k:v for k,v in train_info}\n",
    "\n",
    "# Test\n",
    "with open(\"data/SCUT-FBP5500_v2/train_test_files/split_of_60%training and 40%testing/test.txt\",\"r\") as test:\n",
    "    test_info = [l.split() for l in test.readlines()]\n",
    "    test_info = {k:v for k,v in test_info}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# let's load in all our dataframes ( 10gb) from 01_Unsupervised_PCA_ML_pipeline.ipynb\n",
    "path = 'data/SCUT-FBP5500_v2/reduced/'\n",
    "fl = sorted(os.listdir(path))\n",
    "fl.remove('.DS_Store') \n",
    "\n",
    "comp_df = pd.DataFrame()\n",
    "for _, p in enumerate(fl):\n",
    "    comp_df = pd.concat([comp_df,pd.read_pickle(path + p)])\n",
    "print(f'done')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# this cell is going to go through each row and check if the filename is in the train list\n",
    "train_df_list = []\n",
    "test_df_list = []\n",
    "for ix, row in comp_df.iterrows():\n",
    "    if row[0] in train_info.keys():\n",
    "        train_df_list.append(row)\n",
    "    else:\n",
    "        test_df_list.append(row)\n",
    "\n",
    "# okay let's check the length to make sure we grabbed everything\n",
    "len(train_df_list) == len(train_info)        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# now we construct the dataframes\n",
    "train_df = pd.DataFrame(train_df_list, columns=comp_df.columns)\n",
    "test_df = pd.DataFrame(test_df_list, columns=comp_df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# now delete the comp_df to reduce the overhead\n",
    "del comp_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# show feature names for reference\n",
    "train_df.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Filename', 'orb_kp', 'orb_dec', 'male', 'asian', 'PCA_1', 'PCA_2'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SIFT ORB: Visual Concept Detection\n",
    "## The bag-of-features models is one of the most popular and promising approaches for extracting the underlying semantics from image databases. Essentially this classification approach borrows from the bag-of-words concept and uses visual keypoints as features,\"words\", to build a dictionary of visual descriptors for an image class database. We will then use this dictionary to model \"topics\" of an image. We hope that these topics will give us a paradigm in which we can analyze beauty.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Create a helper function\n",
    "def orb_extractor(df):\n",
    "    \"\"\"\n",
    "    Helper function to create a list of all keypoints in a image database \n",
    "    and a dictionary that maps the keypoints (value) to the image (key)\n",
    "    \n",
    "    Params:\n",
    "    df is the dataframe holding the image database\n",
    "    \n",
    "    Return:\n",
    "    [0] image_vectors : dict object with image filename as key and keypoints as values\n",
    "    [1] descriptor_list : a list of all the keypoints in the database\n",
    "    \"\"\"\n",
    "    \n",
    "    image_vectors = {}\n",
    "    descriptor_list =[]\n",
    "    for _, row in df.iterrows():\n",
    "        descriptor_list.extend(row['orb_dec'])\n",
    "        image_vectors[row['Filename']] = row['orb_dec']\n",
    "        \n",
    "    return (image_vectors, descriptor_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# okay now we are going to extract what we need for feature engineering\n",
    "train_orb_dict, train_orbs = orb_extractor(train_df)\n",
    "test_orb_dict, test_orbs = orb_extractor(test_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# before we go any further let's save some files\n",
    "with open('data/SCUT-FBP5500_v2/train_bovw_list', 'wb') as d:\n",
    "    pickle.dump(train_orbs,d)\n",
    "    \n",
    "with open('data/SCUT-FBP5500_v2/train_orb_dict', 'wb') as d:\n",
    "    pickle.dump(train_orb_dict,d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervised classification using Kmeans\n",
    "## We have extracted 631,264 feature vectors (words) for our images\n",
    "### Next we will take these features, words, and model them into topics via clustering. Essentially, we will plot all the keypoints and then find N cluster centers around the keypoints. Each cluster center represents a \"topic\" each topic has many keypoints that model it's semantics.\n",
    "\n",
    "### We use Kmeans for clustering here instead of a density based approach because we believe that the outliers that are lost in density based clustering might be very informative in our beauty paradigm. Conceptually think of these outliers as rarely used words that strongly reference a certain topic.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Only use training data for dictionary creation otherwise there is leakage\n",
    "train_orbs = pickle.load(open('data/SCUT-FBP5500_v2/train_bovw_list', 'rb'))\n",
    "train_orb_dict = pickle.load(open('data/SCUT-FBP5500_v2/train_orb_dict', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# A k-means clustering algorithm takes 2 parameter which is number \n",
    "# of cluster(k) and the other is descriptors list(unordered 1d array)\n",
    "# Returns an array that holds central points.\n",
    "def kmeans(k, descriptor_list):\n",
    "    kmeans = KMeans(n_clusters = k, n_init=10)\n",
    "    kmeans.fit(descriptor_list)\n",
    "    visual_words = kmeans.cluster_centers_ \n",
    "    return visual_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Takes the central points which is visual words \n",
    "# Since this is one subject we can greatly reduce this space\n",
    "# let's start with the # of original pts ie 86 \n",
    "visual_words = kmeans(86, train_orbs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# now let's assume everyone a single identifying property\n",
    "# this K is too high for Kmeans and our dataset\n",
    "#visual_words_2200 = kmeans(2200, train_orbs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We landed on representing 500 topics for our image database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# We landed on representing 500 topics\n",
    "# this variable holds the cluster centers of each topic \n",
    "visual_words_500 = kmeans(500, train_orbs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# this cell exports our cluster centers\n",
    "# do not uncomment as this will overwrite a timely process\n",
    "#with open('data/SCUT-FBP5500_v2/visual_words_500_centers', 'wb') as d:\n",
    "#    pickle.dump(visual_words_500,d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Now time for to generate topic frequency for each image –– analyzing BOVF (Bag Of Visual Features).\n",
    "def image_class_normed(all_bovw, centers):\n",
    "    \"\"\"\n",
    "    This is a helper function that analyzes the euclidean distance between every keypoint in an image \n",
    "    and the topic cluster centers. It generates a histogram for each topic representation in an image.\n",
    "    Essentially, this is generating a document term frequency matrix but for an image.\n",
    "    \n",
    "    Params:\n",
    "    all_bovw: A dictionary that holds the keypoints as values and is separated by image filename key \n",
    "    centers: An array that holds the central points (visual topics) of the k means clustering\n",
    "    \n",
    "    Return:\n",
    "    feat_dict : dictionary that holds the histograms for each images. \n",
    "    \n",
    "    \"\"\"\n",
    "    dict_keys = []\n",
    "    feats = []\n",
    "    for key,value in all_bovw.items():\n",
    "        dict_keys.append(key)\n",
    "        # obtains distance/closeness to centers for keypoints\n",
    "        dist = distance.cdist(value, centers, metric='euclidean')\n",
    "        \n",
    "        # argmin for each of key points, get the closest feature vocab (center)\n",
    "        bin_assignment = np.argmin(dist, axis=1)\n",
    "        \n",
    "        # classify each kp into symbols\n",
    "        # create histogram with size N describing number of symbols\n",
    "        histogram = np.zeros(len(centers))\n",
    "        for id_assign in bin_assignment:\n",
    "            histogram[id_assign] += 1\n",
    "        \n",
    "        # assign the histogram to global features    \n",
    "        feats.append(histogram)\n",
    "    \n",
    "    # normalize \n",
    "    feats = np.asarray(feats)\n",
    "    feats_norm = np.linalg.norm(feats,axis=1)\n",
    "    for i in range(0, feats.shape[0]):\n",
    "        feats[i] = feats[i] /feats_norm[i]\n",
    "        \n",
    "    # feats now holds all the image features\n",
    "    feat_dict = {k:v for k,v in zip(dict_keys, feats)}\n",
    "\n",
    "    return feat_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Creates histograms for train data  \n",
    "# returns dict with image as key then a matrix of visual features  \n",
    "bovw_train = image_class_normed(train_orb_dict, visual_words_500) \n",
    "\n",
    "# Creates histograms for test data\n",
    "bovw_test = image_class_normed(test_orb_dict, visual_words_500)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "# map agg user ratings to tain data\n",
    "train_df['rating'] = train_df['Filename'].apply(lambda x: train_info[x])\n",
    "# assign BOVF feature to train df\n",
    "train_df['bovw'] = train_df['Filename'].apply(lambda x: bovw_train[x])\n",
    "\n",
    "# assign BOVF feature to test df\n",
    "test_df['bovw'] = test_df['Filename'].apply(lambda x: bovw_test[x])\n",
    "# map agg user ratings to test data\n",
    "test_df['rating'] = test_df['Filename'].apply(lambda x: test_info[x])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "train_df_flt = train_df[['Filename', 'male', 'asian','bovw','PCA_1', 'PCA_2','rating']]\n",
    "test_df_flt = test_df[['Filename', 'male', 'asian','bovw','PCA_1', 'PCA_2','rating']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# save our files\n",
    "with open('data/SCUT-FBP5500_v2/train_df', 'wb') as d:\n",
    "    pickle.dump(train_df_flt,d)\n",
    "    \n",
    "with open('data/SCUT-FBP5500_v2/test_df', 'wb') as d:\n",
    "    pickle.dump(test_df_flt,d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Discovered that our feature mapping didn't work for our binary variables\n",
    "# correcting that here\n",
    "test_df_flt['male'] = test_df_flt['male'].fillna(0)\n",
    "test_df_flt['asian'] = test_df_flt['asian'].fillna(0)\n",
    "\n",
    "train_df_flt['male'] = train_df_flt['male'].fillna(0)\n",
    "train_df_flt['asian'] = train_df_flt['asian'].fillna(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Steps\n",
    "## We need to explode our feature arrays into columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Data: All Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = 250  #chunk row size\n",
    "list_df_train = [train_df_flt[i:i+n] for i in range(0,train_df_flt.shape[0],n)]\n",
    "# need to unravel all the columns with matrices\n",
    "\n",
    "#del train_df\n",
    "# set compression\n",
    "compression_opts = dict(method='zip',archive_name='out.csv')\n",
    "\n",
    "for ix, batch in enumerate(list_df):\n",
    "\n",
    "      print(f\"\\nstarting batch {ix}\")\n",
    "      # hold row in list bc concat expense\n",
    "      tmp_df = []\n",
    "      # flatten arrays into columns\n",
    "      for i,row in batch.iterrows():\n",
    "          tmp = []\n",
    "          tmp.append(row['rating'])\n",
    "          tmp.append(row['Filename'])\n",
    "          tmp.append(row['male'])\n",
    "          tmp.append(row['asian'])\n",
    "          tmp.extend(np.ravel(row['bovw']))\n",
    "          tmp.extend(np.ravel(row['PCA_1']))\n",
    "          tmp.extend(np.ravel(row['PCA_2']))\n",
    "          tmp_df.append(tmp)\n",
    "\n",
    "      # build output\n",
    "      rebuilt = pd.DataFrame(tmp_df)  \n",
    "      rebuilt.rename(columns={0:'rating',1:'filename',2:'male',3:'asian'}, inplace= True) \n",
    "\n",
    "      # Save output\n",
    "      with open(f\"data/SCUT-FBP5500_v2/train_batch_00{ix}.zip\", 'wb') as out:\n",
    "        rebuilt.to_csv(out, index=False,compression=compression_opts)\n",
    "\n",
    "      print(f'done with batch {ix}\\n')    \n",
    "\n",
    "#rebuilt = pd.concat([rebuilt, pd.DataFrame([tmp])])\n",
    "#"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Data: Bag of Visual Features and no PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this is a version with only bovw\n",
    "tmp_df = []\n",
    "# flatten arrays into columns\n",
    "for i,row in train_df_flt.iterrows():\n",
    "    tmp = []\n",
    "    tmp.append(row['rating'])\n",
    "    tmp.append(row['Filename'])\n",
    "    tmp.append(row['male'])\n",
    "    tmp.append(row['asian'])\n",
    "    tmp.extend(np.ravel(row['bovw']))\n",
    "    tmp_df.append(tmp)\n",
    "\n",
    "# build output\n",
    "rebuilt = pd.DataFrame(tmp_df)  \n",
    "rebuilt.rename(columns={0:'rating',1:'filename',2:'male',3:'asian'}, inplace= True) \n",
    "\n",
    "# Save output\n",
    "with open(f\"data/SCUT-FBP5500_v2/train_bovw.zip\", 'wb') as out:\n",
    "  rebuilt.to_csv(out, index=False,compression=compression_opts)\n",
    "print('done')   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Data: All Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "n = 250  #chunk row size\n",
    "list_df_test = [test_df_flt[i:i+n] for i in range(0,test_df_flt.shape[0],n)]\n",
    "# need to unravel all the columns with matrices\n",
    "\n",
    "#del train_df\n",
    "# set compression\n",
    "compression_opts = dict(method='zip',archive_name='out.csv')\n",
    "\n",
    "for ix, batch in enumerate(list_df_test):\n",
    "\n",
    "      print(f\"\\nstarting batch {ix}\")\n",
    "      # hold row in list bc concat expense\n",
    "      tmp_df = []\n",
    "      # flatten arrays into columns\n",
    "      for i,row in batch.iterrows():\n",
    "          tmp = []\n",
    "          tmp.append(row['rating'])\n",
    "          tmp.append(row['Filename'])\n",
    "          tmp.append(row['male'])\n",
    "          tmp.append(row['asian'])\n",
    "          tmp.extend(np.ravel(row['bovw']))\n",
    "          tmp.extend(np.ravel(row['PCA_1']))\n",
    "          tmp.extend(np.ravel(row['PCA_2']))\n",
    "          tmp_df.append(tmp)\n",
    "\n",
    "      # build output\n",
    "      rebuilt = pd.DataFrame(tmp_df)  \n",
    "      rebuilt.rename(columns={0:'rating',1:'filename',2:'male',3:'asian'}, inplace= True) \n",
    "\n",
    "      # Save output\n",
    "      with open(f\"data/SCUT-FBP5500_v2/test_batch_00{ix}.zip\", 'wb') as out:\n",
    "        rebuilt.to_csv(out, index=False,compression=compression_opts)\n",
    "\n",
    "      print(f'done with batch {ix}\\n')    \n",
    "\n",
    "#rebuilt = pd.concat([rebuilt, pd.DataFrame([tmp])])\n",
    "#"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2200, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Data: Bag of Visual Features and no PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this is a version with only bovw\n",
    "tmp_df = []\n",
    "# flatten arrays into columns\n",
    "for i,row in test_df_flt.iterrows():\n",
    "    tmp = []\n",
    "    tmp.append(row['rating'])\n",
    "    tmp.append(row['Filename'])\n",
    "    tmp.append(row['male'])\n",
    "    tmp.append(row['asian'])\n",
    "    tmp.extend(np.ravel(row['bovw']))\n",
    "    tmp_df.append(tmp)\n",
    "\n",
    "# build output\n",
    "rebuilt = pd.DataFrame(tmp_df)  \n",
    "rebuilt.rename(columns={0:'rating',1:'filename',2:'male',3:'asian'}, inplace= True) \n",
    "\n",
    "# Save output\n",
    "with open(f\"data/SCUT-FBP5500_v2/test_bovw.zip\", 'wb') as out:\n",
    "  rebuilt.to_csv(out, index=False,compression=compression_opts)\n",
    "print('done')   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Done"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}